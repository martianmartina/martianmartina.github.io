---
layout: page
title: Neural Seq2seq Grammars with Heuristic Alignments
description: Undergraduate thesis, Spring 2023
img: assets/img/qcfg.jpg
importance: 2
category: Research
---
#### TL;DR:
 Neural QCFG (quasi-synchronous context-free grammars) with heuristic alignments from pretrained models.

Stay Tuned!

<!-- Linguistics has long been modeling structures in languages as trees, assuming the **compositionality** in language.
> ###### *The meaning of a compound expression is a function of the meanings of its parts and of the way they are syntactically combined.*

Compositionality seems to be a reasonable inductive bias for generalization. Rather than having the model memorize an infinite number of instances, we want it to learn the rules that generate them. 

Nevertheless, most neural networks fail spectacularly on compositional generalization tests, partially due to a weak inductive bias that tends to capture surface-level statistics rather than a hierarchical latent structure. 

To address this issue, I investigated neural Quasi-synchronous Context-Free Grammar (QCFG) for sequence-to-sequence learning. It induces latent trees and explicitly models the alignment between source and target trees for better compositional generalizability and interpretability.

I also improved it using posterior regularization to better align source and target non-terminal nodes. The regularization is guided by a pretrained multilingual language model, which encourages alignments between two spans whose embeddings are close in the pretrained latent space.  -->
